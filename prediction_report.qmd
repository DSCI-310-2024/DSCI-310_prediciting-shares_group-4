---
title: Exploratory Data Analysis
jupyter: ir
---

```{r}
#| vscode: {languageId: r}
# Assuming your kernel autoinstalled renv, uncomment the following code
# to install the rest of the packages
# renv::restore()
```

Title: Predicting number of shares an article will get based on the number of links, images and videos in the article.
Summary: Our analysis aims to predict the number of shares an article published by Mashable over a period of two years will get based on the number of links, images and videos in the article using knn classification. An article is classified as popular if it has equal to or more than 1400 shares and unpopular if it has less than 1400 shares. Our finisdings suggest that ...

Loading relevant packages.
```{r}
#| vscode: {languageId: r}
library(kknn)
library(renv)
library(tidymodels)
library(tidyverse)
library(repr)
```

Reading data from dataset.
```{r}
#| vscode: {languageId: r}
articles <- read_csv('data/OnlineNewsPopularity.csv')
```

Creating a boxplot of the variable of interest
```{r}
#| vscode: {languageId: r}
ggplot(data = articles) +
    geom_boxplot(aes(y = shares)) +
    labs(title = "Boxplot of Shares") +
    ylab(label = 'Shares') +
    theme(axis.title = element_text(size = 20), axis.text = element_text(size = 15), title = element_text(size = 25))
```

Obtaining summary statistics about the variable of interest.
```{r}
#| vscode: {languageId: r}
summary(articles$shares)
sd(articles$shares)
```

The variable of interest, shares, has a median of 1400 and a mean of 3395. The distribution spread is quite large, with a standard deviation of 11627. We will be using knn classification to determine whether an article will be popular. We will create a new variable called is_popular with two values 1 (popular) and 0 (unpopular)". An article is classified as popular if it has equal to or more than 1400 shares and unpopular if it has less than 1400 shares.

```{r}
#| vscode: {languageId: r}
articles_clean <- articles |>
      select(shares, num_hrefs, num_imgs, num_videos) |>
      mutate(is_popular = ifelse(shares < 1400, 0, 1)) |>
      mutate(is_popular = as.factor(is_popular))

head(articles_clean)
```

```{r}
#| vscode: {languageId: r}
write.csv(articles_clean, 'data/clean_Articles.csv')
```

Since we will be performing classification, we split our data into a training and a testing set with a 60% proportion.
```{r}
#| vscode: {languageId: r}
set.seed(2024)

articles_data_split <- initial_split(articles_clean, prop = 0.6, strata = is_popular)
articles_training <- training(articles_data_split)
articles_testing <- testing(articles_data_split)
```

We start our preliminary data analysis process by examining the number of observations we have in the training set for each class.

```{r}
num_obs_training <- articles_training |>
      group_by(is_popular) |>
      summarize(n = n()) |>
      mutate(percentage = 100*n/nrow(articles_training))

num_obs_training
```

We can see that our data is balanced and since we have close percentages of data in both popular and unpopular classes, with popular holding the majority at 53.4%.

Next, we visualize the distributions of our predictor variables: num_hrefs, num_imgs, and num_videos. We use is_popular as the fill argument.

```{r}
# Histogram 1: Distribution of the number of links in article
mean_hrefs_plot <- ggplot(articles_training, aes(x = num_hrefs, fill = is_popular)) +
  geom_histogram(color = "black") +
  labs(title = "Distribution of number of links", 
       x = "Number of links", 
       fill = "Popular")     
mean_hrefs_plot

# Histogram 1: Distribution of the number of images in article
mean_imgs_plot <- ggplot(articles_training, aes(x = num_imgs, fill = is_popular)) +
  geom_histogram(color = "black") +
  labs(title = "Distribution of number of images", 
       x = "Number of images", 
       fill = "Popular")     
mean_imgs_plot

# Histogram 1: Distribution of the number of videos in article
mean_videos_plot <- ggplot(articles_training, aes(x = num_videos, fill = is_popular)) +
  geom_histogram(color = "black") +
  labs(title = "Distribution of number of videos", 
       x = "Number of videos", 
       fill = "Popular")     
mean_videos_plot
```

Next, we start building our knn classification model.

We start with making a recipe using the training data.
```{r}
articles_recipe <- recipe(is_popular ~ num_hrefs + num_imgs + num_videos, data = articles_training) |>
    step_scale(all_predictors()) |>
    step_center(all_predictors())
articles_recipe
```

Next, we build a tuning model for picking the best value of k.

```{r}
tune_spec <- nearest_neighbor(weight_func = "rectangular", neighbors = tune()) |>
    set_engine("kknn") |>
    set_mode("classification")
```

Next, we perform cross-validation and create a workflow that calculate the metrics for each of the K values 1, 6, ..., 46, and then return a data frame that shows the accuracy of each K value.

```{r}
set.seed(2023) # set the seed

# cross-validation
articles_vfold <- vfold_cv(articles_training, v = 5, strata = is_popular)

# create a set of K values
kvals <- tibble(neighbors = seq(from = 1, to = 100, by = 5))

# data analysis workflow                       
    knn_results <- workflow() |>
    add_recipe(articles_recipe) |>
    add_model(tune_spec) |>
    tune_grid(resamples = articles_vfold, grid = kvals) |>
    collect_metrics()

accuracies <- knn_results |>
    filter(.metric == "accuracy")

print(accuracies)
```

We now work on making an accuracy vs k plot.

```{r}
best_k_plot <- accuracies |>
    ggplot(aes(x = neighbors, y = mean)) +
    geom_point() +
    geom_line() +
    labs(x = "Number of neighbors", y = "Accuracy Estimate") +
    ggtitle("Accuracy Estimates vs. Number of Neighbors")
best_k_plot
```